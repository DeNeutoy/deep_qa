'''
Knowledge selectors take:
 - encoded representations of background facts related to the sentence
 - attention weights over the background

 These are then combined in some way to return a single representation of the
 background knowledge per sample. The simplest way for this to happen is simply
 taking a weighted average of the knowledge representations with respect to the
 attention weights.

 Input shapes: (samples, knowledge_len, input_dim), (samples, knowledge_len)
 Output shape: (samples, input_dim)
'''


from collections import OrderedDict
from overrides import overrides
import numpy as np

from keras.engine import InputSpec
from keras import backend as K
from keras.layers.recurrent import GRU, time_distributed_dense
from keras.layers import merge


class WeightedAverageKnowledgeCombiner:

    def __init__(self, knowledge_axis):
        self.knowledge_axis = knowledge_axis

    def __call__(self, encoded_knowledge, attention_weights, index):

        weighted_average = lambda avg_inputs: K.sum(avg_inputs[0] * K.expand_dims(avg_inputs[1], dim=-1),
                                                    axis=self.knowledge_axis)

        # input shapes: (samples, knowledge_len, input_dim), (samples, knowledge_len)
        # output shape: (samples, input_dim)
        weighted_average_shape = self.get_weighted_average_shape()
        attended_knowledge = merge([encoded_knowledge, attention_weights],
                                   mode=weighted_average,
                                   output_shape=weighted_average_shape,
                                   name='background_weighted_average_%d' % index)

        return attended_knowledge

    def get_weighted_average_shape(self):

        def merged_shape(input_shapes):
            shape = [x for x in input_shapes[0]]
            shape.pop(self.knowledge_axis)
            return tuple(shape)

        return merged_shape


class AttentionBasedGRUKnowledgeCombiner:
    '''
     Input shapes: (samples, knowledge_len, input_dim), (samples, knowledge_len)
    Output shape: (samples, input_dim)

    This Knowledge Selector runs an Attentive GRU over the knowledge encoding. It returns
    a representation of the knowledge generated by taking the last state of the GRU after
    running over every piece of knowledge.

    '''
    def __init__(self, knowledge_axis, attention_GRU):
        self.knowledge_axis = knowledge_axis
        self.attention_GRU = attention_GRU

    def __call__(self, encoded_knowledge, attention_weights, index, mask=None):

        '''
        Calls to Keras' Recurrent API require the input of a single tensor.
        In order to use the attention weights inside the GRU architecture,
        we need to pass in a single tensor containing the inputs(in this case,
        the background knowledge vectors) AND the attention weights, which are prepended.

         (samples, knowledge_len), (samples, knowledge_len, input_dim)
                                    => (samples, knowledge_len, 1 + input_dim)
        '''

        # TODO: this merge assumes the word dim is in the dimension after the knowledge axis. Is this ok? YES!
        gru_input = merge([K.expand_dims(attention_weights, dim=self.knowledge_axis + 1), encoded_knowledge],
                          mode='concat',
                          concat_axis= self.knowledge_axis + 1,
                          name='concat_attention_with_background_%d' % index)

        last_state = self.attention_GRU(gru_input)

        return last_state


class AttentiveGRU(GRU):

    '''
    This AttentiveGRU class extends the Keras Gated Recurrent Unit by implementing a method which substitutes
    the GRU update gate(normally a vector, z - it is noted below where it is normally computed) for a scalar
    attention weight(one per input, such as from the output of a softmax over the input vectors), which is
    pre-computed. In this specific case, the vectors are the encoded background knowledge in a memory network.

    The implementation of this class is subtle and is designed for use with the AttentionBasedGRUKnowledgeEncoder.
    When it is initialised, the Keras backend will call the build method. It uses this to check that inputs being
    passed to this function are the correct size, so we allow this to be the actual input size as normal.

    However, for the internal implementation, everywhere where this global shape is used, we override it to be one
    less, as we are passing in a tensor of shape (batch, knowledge_length, 1 + encoding_dim) as we are including
    the attention mask. Therefore, we need all of the weights to have shape(*, encoding_dim), NOT (*, 1 + encoding_dim).
    All of the below methods which are overridden use some form of this dimension, so we correct them.


    '''

    def __init__(self, encoding_dim, name="attentive_gru", **kwargs):
        super(AttentiveGRU, self).__init__(encoding_dim, name=name, **kwargs)

    @overrides
    def step(self, x, states):

        '''
        The input to step is a tensor of shape (batch, 1 + encoding_dim), ie a timeslice of
        the input to this AttentiveGRU. Before we start, we strip off the attention from the
        beginning. Then we do the equations for a normal GRU, except we don't calculate the
        output gate z, substituting the attention mask for it instead.
        '''

        attention = x[:, 1]
        x = x[:, 1:]
        h_tm1 = states[0]  # previous memory
        B_U = states[1]  # dropout matrices for recurrent units
        B_W = states[2]

        if self.consume_less == 'gpu':

            matrix_x = K.dot(x * B_W[0], self.W) + self.b
            matrix_inner = K.dot(h_tm1 * B_U[0], self.U[:, :2 * self.output_dim])

            x_r = matrix_x[:, self.output_dim: 2 * self.output_dim]
            inner_r = matrix_inner[:, self.output_dim: 2 * self.output_dim]

            # Here we would normally have computed a z output gate.
            # Instead we use our attention mask.
            # z = self.inner_activation(x_z + inner_z)
            r = self.inner_activation(x_r + inner_r)

            x_h = matrix_x[:, 2 * self.output_dim:]
            inner_h = K.dot(r * h_tm1 * B_U[0], self.U[:, 2 * self.output_dim:])
            hh = self.activation(x_h + inner_h)
        else:
            if self.consume_less == 'cpu':
                x_r = x[:, self.output_dim: 2 * self.output_dim]
                x_h = x[:, 2 * self.output_dim:]
            elif self.consume_less == 'mem':
                x_r = K.dot(x * B_W[1], self.W_r) + self.b_r
                x_h = K.dot(x * B_W[2], self.W_h) + self.b_h
            else:
                raise Exception('Unknown `consume_less` mode.')

            # Here we would normally have computed a z output gate.
            # Instead we use our attention mask.
            # z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))
            r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))

            hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))

        # Here is the KEY difference between a GRU and an AttentiveGRU. Instead of using
        # a learnt output gate(z), we use a scalar attention vector (batch, 1) for this
        # particular background knowledge vector.
        h = attention * hh + (1 - attention) * h_tm1
        return h, [h]

    def build(self, input_shape):
        '''
        This is used by Keras to verify things, but also to build the weights.
        The only difference from the Keras GRU here is that we generate weights
        with dimension input_dim[2] - 1, rather than dimension input_dim[2].
        '''

        self.input_spec = [InputSpec(shape=input_shape)]

        # Here we make all the weights with a dimension one smaller
        # than the input, as we remove the attention masks.
        self.input_dim = input_shape[2] - 1

        if self.stateful:
            self.reset_states()
        else:
            # initial states: all-zero tensor of shape (output_dim)
            self.states = [None]

        if self.consume_less == 'gpu':

            self.W = self.init((self.input_dim, 3 * self.output_dim),
                               name='{}_W'.format(self.name))
            self.U = self.inner_init((self.output_dim, 3 * self.output_dim),
                                     name='{}_U'.format(self.name))

            self.b = K.variable(np.hstack((np.zeros(self.output_dim),
                                           np.zeros(self.output_dim),
                                           np.zeros(self.output_dim))),
                                name='{}_b'.format(self.name))

            self.trainable_weights = [self.W, self.U, self.b]
        else:

            self.W_z = self.init((self.input_dim, self.output_dim),
                                 name='{}_W_z'.format(self.name))
            self.U_z = self.inner_init((self.output_dim, self.output_dim),
                                       name='{}_U_z'.format(self.name))
            self.b_z = K.zeros((self.output_dim,), name='{}_b_z'.format(self.name))

            self.W_r = self.init((self.input_dim, self.output_dim),
                                 name='{}_W_r'.format(self.name))
            self.U_r = self.inner_init((self.output_dim, self.output_dim),
                                       name='{}_U_r'.format(self.name))
            self.b_r = K.zeros((self.output_dim,), name='{}_b_r'.format(self.name))

            self.W_h = self.init((self.input_dim, self.output_dim),
                                 name='{}_W_h'.format(self.name))
            self.U_h = self.inner_init((self.output_dim, self.output_dim),
                                       name='{}_U_h'.format(self.name))
            self.b_h = K.zeros((self.output_dim,), name='{}_b_h'.format(self.name))

            self.trainable_weights = [self.W_z, self.U_z, self.b_z,
                                      self.W_r, self.U_r, self.b_r,
                                      self.W_h, self.U_h, self.b_h]

            self.W = K.concatenate([self.W_z, self.W_r, self.W_h])
            self.U = K.concatenate([self.U_z, self.U_r, self.U_h])
            self.b = K.concatenate([self.b_z, self.b_r, self.b_h])

        self.regularizers = []
        if self.W_regularizer:
            self.W_regularizer.set_param(self.W)
            self.regularizers.append(self.W_regularizer)
        if self.U_regularizer:
            self.U_regularizer.set_param(self.U)
            self.regularizers.append(self.U_regularizer)
        if self.b_regularizer:
            self.b_regularizer.set_param(self.b)
            self.regularizers.append(self.b_regularizer)

        if self.initial_weights is not None:
            self.set_weights(self.initial_weights)
            del self.initial_weights

    @overrides
    def preprocess_input(self, x):
        '''
        We have to override this preprocessing step, because if we are using the cpu,
        we do the weight - input multiplications in the internals of the GRU as seperate,
        smaller matrix multiplications and concatenate them after. Therefore, before this
        happens, we split off the attention and then add it back afterwards.
        '''

        if self.consume_less == 'cpu':

            attention = x[:, :, 1]  # (samples, knowledge_length)
            x = x[:, :, 1:] # (samples, knowledge_length, word_dim)

            input_shape = self.input_spec[0].shape
            input_dim = input_shape[2]
            timesteps = input_shape[1]

            x_z = time_distributed_dense(x, self.W_z, self.b_z, self.dropout_W,
                                         input_dim, self.output_dim, timesteps)
            x_r = time_distributed_dense(x, self.W_r, self.b_r, self.dropout_W,
                                         input_dim, self.output_dim, timesteps)
            x_h = time_distributed_dense(x, self.W_h, self.b_h, self.dropout_W,
                                         input_dim, self.output_dim, timesteps)

            return K.concatenate([K.expand_dims(attention, 2), x_z, x_r, x_h], axis=2)
        else:
            return x

# The first item added here will be used as the default in some cases.
knowledge_combiners = OrderedDict()  # pylint:  disable=invalid-name
knowledge_combiners["weighted_average"] = WeightedAverageKnowledgeCombiner
knowledge_combiners["attentive_GRU"] = AttentionBasedGRUKnowledgeCombiner

